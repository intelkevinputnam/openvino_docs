
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Using AI Accelerators {#ovms_docs_target_devices} &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../../../../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../../../../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/tabs.css" type="text/css" />
    <script src="../../../../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../../../../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../../../../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/tabs.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/js/custom.js"></script>
    <script src="../../../../_static/js/graphs.js"></script>
    <script src="../../../../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script src="../../../../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/pages/documentation/ecosystem/model-server/accelerators.html" />
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../../index.html">
  <img src="../../../../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../get-started-guide.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../resources.html">
  Resources
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../languages/zh_CN/index.html">
  简体中文
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/openvino_docs/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/openvino_docs/languages/zh_CN/index.html">简体中文 (Simplified Chinese)</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../../../../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#starting-the-server-with-the-intel-neural-compute-stick-2">
   Starting the server with the Intel® Neural Compute Stick 2
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#starting-a-docker-container-with-hddl">
   Starting a Docker Container with HDDL
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#starting-a-docker-container-with-intel-gpu">
   Starting a Docker Container with Intel GPU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-multi-device-plugin">
   Using Multi-Device Plugin
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-heterogeneous-plugin">
   Using Heterogeneous Plugin
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-auto-plugin">
   Using AUTO Plugin
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="using-ai-accelerators-ovms-docs-target-devices">
<h1>Using AI Accelerators {#ovms_docs_target_devices}<a class="headerlink" href="#using-ai-accelerators-ovms-docs-target-devices" title="Permalink to this headline">¶</a></h1>
<section id="starting-the-server-with-the-intel-neural-compute-stick-2">
<h2>Starting the server with the Intel® Neural Compute Stick 2<a class="headerlink" href="#starting-the-server-with-the-intel-neural-compute-stick-2" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://software.intel.com/en-us/neural-compute-stick">Intel Movidius Neural Compute Stick 2</a> can be employed by OVMS OpenVINO Model Server via
<a class="reference external" href="https://docs.openvino.ai/2022.1/openvino_docs_OV_UG_supported_plugins_MYRIAD.html">the MYRIAD plugin</a>. It must be visible and accessible on the host machine.</p>
<p>NCS devices should be reported by the <code class="docutils literal notranslate"><span class="pre">lsusb</span></code> command, printing out <code class="docutils literal notranslate"><span class="pre">ID</span> <span class="pre">03e7:2485</span></code>.</p>
<p>To start the server with Neural Compute Stick use either of the two options:</p>
<ol class="arabic">
<li><p>More securely, without the docker privileged mode and mounting only the usb devices.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker run --rm -it -u <span class="m">0</span> --device-cgroup-rule<span class="o">=</span><span class="s1">&#39;c 189:* rmw&#39;</span> -v /opt/model:/opt/model -v /dev/bus/usb:/dev/bus/usb -p <span class="m">9001</span>:9001 openvino/model_server <span class="se">\</span>
--model_path /opt/model --model_name my_model --port <span class="m">9001</span> --target_device MYRIAD
</pre></div>
</div>
</li>
<li><p>less securely, in the docker privileged mode and mounting all devices.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker run --rm -it --net<span class="o">=</span>host -u root --privileged -v /opt/model:/opt/model -v /dev:/dev -p <span class="m">9001</span>:9001 openvino/model_server <span class="se">\</span>
--model_path /opt/model --model_name my_model --port <span class="m">9001</span> --target_device MYRIAD
</pre></div>
</div>
</li>
</ol>
</section>
<section id="starting-a-docker-container-with-hddl">
<h2>Starting a Docker Container with HDDL<a class="headerlink" href="#starting-a-docker-container-with-hddl" title="Permalink to this headline">¶</a></h2>
<p>To run a container that is using the HDDL accelerator, <em>hddldaemon</em> must be running on the host machine.
You must set up the environment (the OpenVINO package must be pre-installed) and start <em>hddldaemon</em> on the host before starting a container.
Refer to the steps from <a class="reference external" href="https://docs.openvino.ai/2022.1/openvino_docs_install_guides_installing_openvino_docker_linux.html#running-the-image-on-intel-vision-accelerator-design-with-intel-movidius-vpus">OpenVINO installation guides</a>.</p>
<p>An example of a command starting a server with HDDL:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># --device=/dev/ion:/dev/ion mounts the accelerator device</span>
<span class="c1"># -v /var/tmp:/var/tmp enables communication with _hddldaemon_ running on the host machine</span>
docker run --rm -it --device<span class="o">=</span>/dev/ion:/dev/ion -v /var/tmp:/var/tmp -v /opt/model:/opt/model -p <span class="m">9001</span>:9001 openvino/model_server:latest <span class="se">\</span>

--model_path /opt/model --model_name my_model --port <span class="m">9001</span> --target_device HDDL
</pre></div>
</div>
<p>Check out our recommendations for <a class="reference internal" href="performance_tuning.html"><span class="doc std std-doc">throughput optimization on HDDL</span></a>.</p>
<blockquote>
<div><p><strong>NOTE</strong>:
the OpenVINO Model Server process within the container communicates with <em>hddldaemon</em> via unix sockets in the <code class="docutils literal notranslate"><span class="pre">/var/tmp</span></code> folder.
It requires RW permissions in the docker container security context.
It is recommended to start the docker container in the same context as the account starting <em>hddldaemon</em>. For example, if you start the <em>hddldaemon</em> as root, add <code class="docutils literal notranslate"><span class="pre">--user</span> <span class="pre">root</span></code> to the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> command.</p>
</div></blockquote>
</section>
<section id="starting-a-docker-container-with-intel-gpu">
<h2>Starting a Docker Container with Intel GPU<a class="headerlink" href="#starting-a-docker-container-with-intel-gpu" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference external" href="https://docs.openvino.ai/2022.1/openvino_docs_OV_UG_supported_plugins_GPU.html">GPU plugin</a> uses the Intel Compute Library for
Deep Neural Networks (<a class="reference external" href="https://01.org/cldnn">clDNN</a>) to infer deep neural networks. For inference execution, it employs Intel® Processor Graphics including
Intel® HD Graphics, Intel® Iris® Graphics, Intel® Iris® Xe Graphics, and Intel® Iris® Xe MAX graphics.</p>
<p>Before using GPU as OpenVINO Model Server target device, you need to:</p>
<ul class="simple">
<li><p>install the required drivers - refer to <a class="reference external" href="https://docs.openvino.ai/2022.1/openvino_docs_install_guides_installing_openvino_linux.html#step-5-optional-configure-inference-on-non-cpu-devices">OpenVINO installation guide</a></p></li>
<li><p>start the docker container with the additional parameter of <code class="docutils literal notranslate"><span class="pre">--device</span> <span class="pre">/dev/dri</span></code> to pass the device context</p></li>
<li><p>set the parameter of <code class="docutils literal notranslate"><span class="pre">--target_device</span></code> to <code class="docutils literal notranslate"><span class="pre">GPU</span></code>.</p></li>
<li><p>use the <code class="docutils literal notranslate"><span class="pre">openvino/model_server:latest-gpu</span></code> image, which contains GPU dependencies</p></li>
</ul>
<p>A command example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker run --rm -it --device<span class="o">=</span>/dev/dri -v /opt/model:/opt/model -p <span class="m">9001</span>:9001 openvino/model_server:latest-gpu <span class="se">\</span>
--model_path /opt/model --model_name my_model --port <span class="m">9001</span> --target_device GPU
</pre></div>
</div>
<p>Running inference on GPU requires the model server process security context account to have correct permissions. It must belong to the render group identified by the command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>stat -c <span class="s2">&quot;group_name=%G group_id=%g&quot;</span> /dev/dri/render*
</pre></div>
</div>
<p>The default account in the docker image is preconfigured. If you change the security context, use the following command to start the model server container:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker run --rm -it  --device<span class="o">=</span>/dev/dri --group-add<span class="o">=</span><span class="k">$(</span>stat -c <span class="s2">&quot;%g&quot;</span> /dev/dri/render* <span class="p">|</span> head -n <span class="m">1</span><span class="k">)</span> -u <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="se">\</span>

-v /opt/model:/opt/model -p <span class="m">9001</span>:9001 openvino/model_server:latest-gpu <span class="se">\</span>

--model_path /opt/model --model_name my_model --port <span class="m">9001</span> --target_device GPU
</pre></div>
</div>
<blockquote>
<div><p><strong>NOTE</strong>:
The public docker image includes the OpenCL drivers for GPU in version 21.38.21026 (RedHat) and 21.48.21782 (Ubuntu).</p>
</div></blockquote>
<p>Support for <a class="reference external" href="https://www.intel.com/content/www/us/en/architecture-and-technology/visual-technology/arc-discrete-graphics.html">Intel Arc</a>, which is in preview now, requires newer driver version <code class="docutils literal notranslate"><span class="pre">22.10.22597</span></code>. You can build OpenVINO Model server with ubuntu base image and that driver using the command below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make docker_build <span class="nv">INSTALL_DRIVER_VERSION</span><span class="o">=</span><span class="m">22</span>.10.22597
</pre></div>
</div>
</section>
<section id="using-multi-device-plugin">
<h2>Using Multi-Device Plugin<a class="headerlink" href="#using-multi-device-plugin" title="Permalink to this headline">¶</a></h2>
<p>If you have multiple inference devices available (e.g. Myriad VPUs and CPU) you can increase inference throughput by enabling the Multi-Device Plugin.
It distributes Inference requests among multiple devices, balancing out the load. For more detailed information read OpenVINO’s <a class="reference external" href="https://docs.openvino.ai/2022.1/openvino_docs_OV_UG_Running_on_multiple_devices.html">Multi-Device plugin documentation</a> documentation.</p>
<p>To use this feature in OpenVINO Model Server, you can choose one of two ways:</p>
<ol class="arabic simple">
<li><p>Use a .json configuration file to set the <code class="docutils literal notranslate"><span class="pre">--target_device</span></code> parameter with the pattern of: <code class="docutils literal notranslate"><span class="pre">MULTI:&lt;DEVICE_1&gt;,&lt;DEVICE_2&gt;</span></code>.
The order of the devices will define their priority, in this case making <code class="docutils literal notranslate"><span class="pre">device_1</span></code> the primary selection.</p></li>
</ol>
<p>This example of a config.json file sets up the Multi-Device Plugin for a resnet model, using Intel Movidius Neural Compute Stick and CPU as devices:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;model_config_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">   </span><span class="p">{</span><span class="nt">&quot;config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;resnet&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;base_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/opt/ml/resnet&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;target_device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;MULTI:MYRIAD,CPU&quot;</span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}]</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>To start OpenVINO Model Server, with the described config file placed as <code class="docutils literal notranslate"><span class="pre">./models/config.json</span></code>, set the <code class="docutils literal notranslate"><span class="pre">grpc_workers</span></code> parameter to match the <code class="docutils literal notranslate"><span class="pre">nireq</span></code> field in config.json
and use the run command, like so:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker run -d --net=host -u root --privileged --rm -v $(pwd)/models/:/opt/ml:ro -v /dev:/dev -p 9001:9001 \
openvino/model_server:latest --config_path /opt/ml/config.json --port 9001 
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>When using just a single model, you can start OpenVINO Model Server without the config.json file. To do so, use the run command together with additional parameters, like so:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker run -d --net=host -u root --privileged --name ie-serving --rm -v $(pwd)/models/:/opt/ml:ro -v \ 
/dev:/dev -p 9001:9001 openvino/model_server:latest model --model_path /opt/ml/resnet --model_name resnet --port 9001 --target_device &#39;MULTI:MYRIAD,CPU&#39;
</pre></div>
</div>
<p>The deployed model will perform inference on both Intel Movidius Neural Compute Stick and CPU.
The total throughput will be roughly equal to the sum of CPU and Intel Movidius Neural Compute Stick throughputs.</p>
</section>
<section id="using-heterogeneous-plugin">
<h2>Using Heterogeneous Plugin<a class="headerlink" href="#using-heterogeneous-plugin" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference external" href="https://docs.openvino.ai/2022.1/openvino_docs_OV_UG_Hetero_execution.html">HETERO plugin</a> makes it possible to distribute inference load of one model
among several computing devices. That way different parts of the deep learning network can be executed by devices best suited to their type of calculations.
OpenVINO automatically divides the network to optimize the process.</p>
<p>The Heterogenous plugin can be configured using the <code class="docutils literal notranslate"><span class="pre">--target_device</span></code> parameter with the pattern of: <code class="docutils literal notranslate"><span class="pre">HETERO:&lt;DEVICE_1&gt;,&lt;DEVICE_2&gt;</span></code>.
The order of devices will define their priority, in this case making <code class="docutils literal notranslate"><span class="pre">device_1</span></code> the primary and <code class="docutils literal notranslate"><span class="pre">device_2</span></code> the fallback one.</p>
<p>Here is a config example using heterogeneous plugin with GPU as the primary device and CPU as a fallback.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;model_config_list&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">   </span><span class="p">{</span><span class="nt">&quot;config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;resnet&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;base_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/opt/ml/resnet&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;target_device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HETERO:GPU,CPU&quot;</span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}]</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="using-auto-plugin">
<h2>Using AUTO Plugin<a class="headerlink" href="#using-auto-plugin" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://docs.openvino.ai/2022.1/openvino_docs_IE_DG_supported_plugins_AUTO.html">Auto Device</a> (or AUTO in short) is a new special “virtual” or “proxy” device in the OpenVINO toolkit, it doesn’t bind to a specific type of HW device.
AUTO solves the complexity in application required to code a logic for the HW device selection (through HW devices) and then, on the deducing the best optimization settings on that device.
AUTO always chooses the best device, if compiling model fails on this device, AUTO will try to compile it on next best device until one of them succeeds.
Make sure you have passed the devices and access to the devices you want to use in for the docker image. For example with:
<code class="docutils literal notranslate"><span class="pre">--device=/dev/dri</span> <span class="pre">--group-add=$(stat</span> <span class="pre">-c</span> <span class="pre">&quot;%g&quot;</span> <span class="pre">/dev/dri/render*</span> <span class="pre">|</span> <span class="pre">head</span> <span class="pre">-n</span> <span class="pre">1)</span> <span class="pre">-u</span> <span class="pre">$(id</span> <span class="pre">-u):$(id</span> <span class="pre">-g)</span></code></p>
<p>Below is an example of the command with AUTO Plugin as target device. It includes extra docker parameters to enable GPU (/dev/dri) , beside CPU.
&#64;sphinxdirective</p>
<p>.. code-block:: sh</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    docker run --rm -d --device=/dev/dri --group-add=$(stat -c &quot;%g&quot; /dev/dri/render* | head -n 1)\
        -u $(id -u):$(id -g) -v &lt;model_path&gt;:/opt/model -p 9001:9001 openvino/model_server:latest \
        --model_path /opt/model --model_name my_model --port 9001 \
        --target_device AUTO
</pre></div>
</div>
<p>&#64;endsphinxdirective</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Auto</span> <span class="pre">Device</span></code> plugin can also use the <a class="reference internal" href="performance_tuning.html"><span class="doc std std-doc">PERFORMANCE_HINT</span></a> plugin config property that enables you to specify a performance mode for the plugin.</p>
<blockquote>
<div><p><strong>NOTE</strong>: CPU_THROUGHPUT_STREAMS and PERFORMANCE_HINT should not be used together.</p>
</div></blockquote>
<p>To enable Performance Hints for your application, use the following command:
&#64;sphinxdirective</p>
<p>.. tab:: LATENCY</p>
<p>.. code-block:: sh</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    docker run --rm -d --device=/dev/dri --group-add=$(stat -c &quot;%g&quot; /dev/dri/render* | head -n 1) -u $(id -u):$(id -g) \
        -v &lt;model_path&gt;:/opt/model -p 9001:9001 openvino/model_server:latest \
        --model_path /opt/model --model_name my_model --port 9001 \
        --plugin_config &#39;{&quot;PERFORMANCE_HINT&quot;: &quot;LATENCY&quot;}&#39; \
        --target_device AUTO
</pre></div>
</div>
<p>.. tab:: THROUGHTPUT</p>
<p>.. code-block:: sh</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    docker run --rm -d --device=/dev/dri --group-add=$(stat -c &quot;%g&quot; /dev/dri/render* | head -n 1) -u $(id -u):$(id -g) \
        -v &lt;model_path&gt;:/opt/model -p 9001:9001 openvino/model_server:latest \
        --model_path /opt/model --model_name my_model --port 9001 \
        --plugin_config &#39;{&quot;PERFORMANCE_HINT&quot;: &quot;THROUGHTPUT&quot;}&#39; \
        --target_device AUTO
</pre></div>
</div>
<p>&#64;endsphinxdirective</p>
<blockquote>
<div><p><strong>NOTE</strong>: currently, AUTO plugin cannot be used with <code class="docutils literal notranslate"><span class="pre">--shape</span> <span class="pre">auto</span></code> parameter while GPU device is enabled.</p>
</div></blockquote>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>